{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Cosine_Similarity",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "7Cn0jYNXwv6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "debe0f97-b6e1-45c6-cbf1-fed62e6e5d4f"
      },
      "cell_type": "code",
      "source": [
        "!wget -c https://github.com/heukirne/brazilian-blog-dataset/raw/master/corpus.csv.gz -O corpus.csv.gz"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-01-30 23:13:06--  https://github.com/heukirne/brazilian-blog-dataset/raw/master/corpus.csv.gz\n",
            "Resolving github.com (github.com)... 192.30.255.113, 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/heukirne/brazilian-blog-dataset/master/corpus.csv.gz [following]\n",
            "--2019-01-30 23:13:06--  https://raw.githubusercontent.com/heukirne/brazilian-blog-dataset/master/corpus.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 416 Range Not Satisfiable\n",
            "\n",
            "    The file is already fully retrieved; nothing to do.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jemcxd7fw6cg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9d1495aa-9a8e-4494-a570-a71a328fc0a2"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "corpus = pd.read_csv('corpus.csv.gz')\n",
        "corpus.shape, corpus.columns"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1000, 22),\n",
              " Index(['Unnamed: 0', 'id', 'qual_a_melhor_classificao_para_esse_texto',\n",
              "        'qual_a_melhor_classificao_para_esse_texto:confidence', 'authorid',\n",
              "        'blogid', 'content', 'contentcount', 'date', 'firstpart', 'labels',\n",
              "        'me', 'polarity', 'postid', 'published', 'readmore', 'replies',\n",
              "        'rownum', 'title', 'titlecount', 'wpscount', 'year'],\n",
              "       dtype='object'))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "hCfBRO1AxjAt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "d62db6f1-2eb4-4d23-8289-a5d5f4a9f293"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk.stem\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('rslp')\n",
        "stopwords_list = stopwords.words(\"portuguese\")\n",
        "portuguese_stemmer = nltk.stem.RSLPStemmer()\n",
        "stopwords_stem = [portuguese_stemmer.stem(w) for w in stopwords_list]\n",
        "\n",
        "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
        "        return lambda doc: (portuguese_stemmer.stem(w) for w in analyzer(doc))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zx-kyjD6x5FY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a5e5da82-98d2-4a97-9271-87ae47d9ba63"
      },
      "cell_type": "code",
      "source": [
        "data = TfidfVectorizer(max_features=1500, strip_accents='unicode', stop_words=stopwords_stem).fit_transform(corpus.content)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ate', 'estiver', 'estivess', 'ha', 'hao', 'houver', 'houvess', 'ja', 'nao', 'sao', 'so', 'tambem', 'tiver', 'tivess'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "IDCNN5JSyRHO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sim_matrix = cosine_similarity(data.todense(), data.todense())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rrJUKRU1zKB7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "text_graph = nx.from_numpy_matrix(sim_matrix)\n",
        "pr = nx.pagerank(text_graph, alpha=0.9, max_iter=1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TMVsZrZnDBHe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}