{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = pd.read_csv('corpus.csv.gz', compression='gzip')\n",
    "stopwords = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix labels to binary\n",
    "lb = preprocessing.LabelBinarizer(neg_label=1, pos_label=2)\n",
    "target = lb.fit_transform(corpus['qual_a_melhor_classificao_para_esse_texto'].values)\n",
    "c, r = target.shape\n",
    "target = target.reshape(c,)\n",
    "\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: f1(0.8), acc(0.7), recall(0.94)\n",
      "600: f1(0.81), acc(0.71), recall(0.94)\n",
      "700: f1(0.8), acc(0.71), recall(0.94)\n",
      "800: f1(0.8), acc(0.71), recall(0.94)\n",
      "900: f1(0.81), acc(0.72), recall(0.94)\n",
      "1000: f1(0.81), acc(0.72), recall(0.93)\n",
      "1100: f1(0.81), acc(0.72), recall(0.93)\n",
      "1200: f1(0.81), acc(0.72), recall(0.93)\n",
      "1300: f1(0.81), acc(0.72), recall(0.94)\n",
      "1400: f1(0.81), acc(0.72), recall(0.94)\n",
      "1500: f1(0.81), acc(0.72), recall(0.94)\n",
      "1600: f1(0.82), acc(0.73), recall(0.95)\n",
      "1700: f1(0.82), acc(0.73), recall(0.95)\n",
      "1800: f1(0.81), acc(0.72), recall(0.95)\n",
      "1900: f1(0.81), acc(0.72), recall(0.95)\n",
      "2000: f1(0.81), acc(0.72), recall(0.95)\n",
      "2100: f1(0.81), acc(0.72), recall(0.96)\n",
      "2200: f1(0.81), acc(0.72), recall(0.96)\n",
      "2300: f1(0.81), acc(0.72), recall(0.96)\n",
      "2400: f1(0.81), acc(0.72), recall(0.96)\n",
      "2500: f1(0.81), acc(0.72), recall(0.96)\n",
      "2600: f1(0.81), acc(0.72), recall(0.96)\n",
      "2700: f1(0.81), acc(0.71), recall(0.96)\n",
      "2800: f1(0.81), acc(0.72), recall(0.97)\n",
      "2900: f1(0.81), acc(0.71), recall(0.97)\n",
      "3000: f1(0.81), acc(0.71), recall(0.97)\n",
      "3100: f1(0.81), acc(0.71), recall(0.97)\n",
      "3200: f1(0.81), acc(0.71), recall(0.97)\n",
      "3300: f1(0.81), acc(0.71), recall(0.97)\n",
      "3400: f1(0.8), acc(0.7), recall(0.97)\n",
      "3500: f1(0.8), acc(0.7), recall(0.97)\n",
      "3600: f1(0.8), acc(0.7), recall(0.97)\n",
      "3700: f1(0.8), acc(0.7), recall(0.97)\n",
      "3800: f1(0.8), acc(0.7), recall(0.98)\n",
      "3900: f1(0.8), acc(0.7), recall(0.98)\n",
      "4000: f1(0.8), acc(0.7), recall(0.98)\n",
      "4100: f1(0.8), acc(0.7), recall(0.98)\n",
      "4200: f1(0.8), acc(0.7), recall(0.98)\n",
      "4300: f1(0.8), acc(0.7), recall(0.98)\n",
      "4400: f1(0.8), acc(0.7), recall(0.98)\n",
      "4500: f1(0.8), acc(0.69), recall(0.98)\n",
      "4600: f1(0.8), acc(0.7), recall(0.98)\n",
      "4700: f1(0.8), acc(0.7), recall(0.98)\n",
      "4800: f1(0.8), acc(0.69), recall(0.98)\n",
      "4900: f1(0.8), acc(0.69), recall(0.98)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500,5000,100):\n",
    "    data = TfidfVectorizer(max_features=i, strip_accents='unicode', stop_words=stopwords).fit_transform(corpus.content)\n",
    "\n",
    "    f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "    acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "    recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "    \n",
    "    print(str(i) + ': ' + 'f1(' + str(round(f1,2)) \n",
    "          + '), acc(' + str(round(acc,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000: f1(0.8), acc(0.7), recall(0.94)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=500, ngram_range=(1,1), strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "\n",
    "print(str(1000) + ': ' + 'f1(' + str(round(f1,2)) \n",
    "      + '), acc(' + str(round(acc,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'aprendinoenem', u'you', u'clique', u'natal', u'empresa', u'leite', u'produto', u'velho', u'cristo', u'casamento', u'cabelos', u'produtos', u'facebook', u'cores', u'leitura', u'jogo', u'estilo', u'loja', u'pele', u'personagem', u'criancas', u'autor', u'boca', u'festa', u'viagem', u'obrigada', u'linha', u'sonho', u'base', u'ceu', u'jesus', u'musicas', u'igreja', u'morte', u'comeca', u'projeto', u'rua', u'esperanca', u'detalhes', u'mulheres', u'11', u'papel', u'rio', u'estado', u'cabelo', u'internet', u'sol', u'rosto', u'carro', u'quarto', u'pontos', u'felicidade', u'crianca', u'alma', u'www', u'fazia', u'uso', u'fe', u'paz', u'sucesso', u'continuar', u'real', u'terra', u'alto', u'personagens', u'paulo', u'rs', u'leve', u'livros', u'pegar', u'sentimentos', u'amar', u'vivo', u'http', u'relacao', u'questao', u'umas', u'naquele', u'unico', u'tamanho', u'feira', u'problemas', u'grupo', u'ontem', u'alegria', u'ultima', u'passo', u'triste', u'visto', u'30', u'criar', u'faca', u'desejo', u'pior', u'12', u'acontece', u'totalmente', u'motivo', u'grandes', u'local']\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(vectorizer.idf_)[::-1]\n",
    "features = vectorizer.get_feature_names()\n",
    "top_n = 100\n",
    "top_features = [features[i] for i in indices[:top_n]]\n",
    "print top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
