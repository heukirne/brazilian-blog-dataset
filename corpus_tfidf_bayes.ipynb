{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = pd.read_csv('corpus.csv.gz', compression='gzip')\n",
    "stopwords = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 23)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus = corpus[corpus['_golden'] == False]\n",
    "corpus = corpus[corpus['qual_a_melhor_classificao_para_esse_texto:confidence'] == 1]\n",
    "corpus = corpus.reset_index()\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diario' 'outro']\n",
      "0    1\n",
      "1   -1\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# fix labels to binary\n",
    "def classFit(x):\n",
    "    if x['qual_a_melhor_classificao_para_esse_texto'] == \"diario\":\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "corpus['class'] = corpus.apply(classFit,axis=1)\n",
    "target = corpus['class'].values\n",
    "\n",
    "print(corpus['qual_a_melhor_classificao_para_esse_texto'].values[:2])\n",
    "print(corpus['class'][:2])\n",
    "\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "portuguese_stemmer = nltk.stem.RSLPStemmer()\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (portuguese_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: f1(0.8419), acc(0.7639), precision(0.7477), recall(0.9637)\n",
      "600: f1(0.8396), acc(0.7603), precision(0.7459), recall(0.961)\n",
      "700: f1(0.8407), acc(0.7604), precision(0.7427), recall(0.9693)\n",
      "800: f1(0.8458), acc(0.7695), precision(0.7507), recall(0.9694)\n",
      "900: f1(0.8458), acc(0.7696), precision(0.7506), recall(0.9694)\n",
      "1000: f1(0.8489), acc(0.775), precision(0.7553), recall(0.9694)\n",
      "1100: f1(0.8494), acc(0.775), precision(0.7544), recall(0.9721)\n",
      "1200: f1(0.8485), acc(0.7713), precision(0.748), recall(0.9806)\n",
      "1300: f1(0.8474), acc(0.7695), precision(0.7464), recall(0.9806)\n",
      "1400: f1(0.8488), acc(0.7713), precision(0.747), recall(0.9833)\n",
      "1500: f1(0.8474), acc(0.7694), precision(0.7466), recall(0.9805)\n",
      "1600: f1(0.8434), acc(0.7622), precision(0.7404), recall(0.9805)\n",
      "1700: f1(0.8417), acc(0.7586), precision(0.7361), recall(0.9833)\n",
      "1800: f1(0.8401), acc(0.755), precision(0.7321), recall(0.9861)\n",
      "1900: f1(0.8381), acc(0.7514), precision(0.729), recall(0.9861)\n",
      "2000: f1(0.8397), acc(0.755), precision(0.733), recall(0.9833)\n",
      "2100: f1(0.8358), acc(0.7477), precision(0.727), recall(0.9833)\n",
      "2200: f1(0.8362), acc(0.7477), precision(0.7261), recall(0.9861)\n",
      "2300: f1(0.8314), acc(0.7387), precision(0.719), recall(0.9861)\n",
      "2400: f1(0.8288), acc(0.7332), precision(0.7136), recall(0.9889)\n",
      "2500: f1(0.8288), acc(0.7332), precision(0.7136), recall(0.9889)\n",
      "2600: f1(0.8283), acc(0.7314), precision(0.7114), recall(0.9917)\n",
      "2700: f1(0.8283), acc(0.7314), precision(0.7114), recall(0.9917)\n",
      "2800: f1(0.8264), acc(0.7278), precision(0.7086), recall(0.9917)\n",
      "2900: f1(0.8254), acc(0.726), precision(0.707), recall(0.9917)\n",
      "3000: f1(0.8263), acc(0.7278), precision(0.7085), recall(0.9917)\n",
      "3100: f1(0.8234), acc(0.7224), precision(0.7041), recall(0.9917)\n",
      "3200: f1(0.8225), acc(0.7205), precision(0.7029), recall(0.9917)\n",
      "3300: f1(0.8235), acc(0.7224), precision(0.7043), recall(0.9917)\n",
      "3400: f1(0.8229), acc(0.7205), precision(0.7021), recall(0.9944)\n",
      "3500: f1(0.822), acc(0.7188), precision(0.7008), recall(0.9944)\n",
      "3600: f1(0.822), acc(0.7188), precision(0.7008), recall(0.9944)\n",
      "3700: f1(0.822), acc(0.7188), precision(0.7008), recall(0.9944)\n",
      "3800: f1(0.8211), acc(0.717), precision(0.6995), recall(0.9944)\n",
      "3900: f1(0.821), acc(0.717), precision(0.6994), recall(0.9944)\n",
      "4000: f1(0.8201), acc(0.7151), precision(0.698), recall(0.9944)\n",
      "4100: f1(0.821), acc(0.717), precision(0.6994), recall(0.9944)\n",
      "4200: f1(0.821), acc(0.717), precision(0.6994), recall(0.9944)\n",
      "4300: f1(0.8192), acc(0.7133), precision(0.6967), recall(0.9944)\n",
      "4400: f1(0.8183), acc(0.7115), precision(0.6955), recall(0.9944)\n",
      "4500: f1(0.8183), acc(0.7115), precision(0.6955), recall(0.9944)\n",
      "4600: f1(0.8196), acc(0.7134), precision(0.6961), recall(0.9972)\n",
      "4700: f1(0.8188), acc(0.7116), precision(0.6949), recall(0.9972)\n",
      "4800: f1(0.8188), acc(0.7116), precision(0.6949), recall(0.9972)\n",
      "4900: f1(0.8179), acc(0.7098), precision(0.6936), recall(0.9972)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500,5000,100):\n",
    "    data = TfidfVectorizer(max_features=i, strip_accents='unicode', stop_words=stopwords).fit_transform(corpus.content)\n",
    "\n",
    "    f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "    acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "    recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "    precision = cross_val_score(model, data.toarray(), target, cv=10, scoring='precision').mean()\n",
    "    \n",
    "    print(str(i) + ': ' + 'f1(' + str(round(f1,4)) \n",
    "          + '), acc(' + str(round(acc,4)) \n",
    "          + '), precision(' + str(round(precision,4)) \n",
    "          + '), recall(' + str(round(recall,4)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000: precision(0.755), acc(0.775), recall(0.969)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1,1), \n",
    "                             strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "precision = cross_val_score(model, data.toarray(), target, cv=10, scoring='precision').mean()\n",
    "acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "\n",
    "print(str(len(vectorizer.get_feature_names())) + ': ' + 'precision(' + str(round(precision,3)) \n",
    "      + '), acc(' + str(round(acc,3)) \n",
    "          + '), recall(' + str(round(recall,3)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "pd.DataFrame(feature_names).to_csv('feature_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outro -8.20494650955 comunidade\n",
      "outro -8.20494650955 fulo\n",
      "outro -8.13604642991 evangelho\n",
      "outro -8.09210655049 advogado\n",
      "outro -8.03779831781 fas\n",
      "outro -8.03308877664 entrevista\n",
      "outro -8.01446887443 pastor\n",
      "outro -7.99725375844 padre\n",
      "outro -7.98529151638 go\n",
      "outro -7.95079808432 oracao\n",
      "outro -7.94455608341 album\n",
      "outro -7.90166897046 nestle\n",
      "outro -7.89980028931 autor\n",
      "outro -7.89176778464 numeros\n",
      "outro -7.88275931406 presidente\n",
      "\n",
      "diario -4.42094494661 nao\n",
      "diario -5.06213613734 pra\n",
      "diario -5.09789174652 voce\n",
      "diario -5.48163994909 dia\n",
      "diario -5.49191115163 ja\n",
      "diario -5.49938636786 tudo\n",
      "diario -5.54233116239 so\n",
      "diario -5.54613759819 ser\n",
      "diario -5.59639170968 ate\n",
      "diario -5.6485491156 bem\n",
      "diario -5.65939553461 vida\n",
      "diario -5.66902291823 aqui\n",
      "diario -5.69878916473 sempre\n",
      "diario -5.74668010173 sao\n",
      "diario -5.75646602873 la\n"
     ]
    }
   ],
   "source": [
    "model.fit(data.toarray(),target)\n",
    "n = 15 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    print (class_labels[0], coef, feat)\n",
    "\n",
    "print()\n",
    "\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800: precision(0.75), acc(0.77), recall(0.97)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1,1), \n",
    "                             strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "model.fit(data.toarray(),target)\n",
    "n = 400 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    if feat not in vocabulary: \n",
    "        vocabulary.append(feat)\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    if feat not in vocabulary: \n",
    "        vocabulary.append(feat)\n",
    "    \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), strip_accents='unicode', \n",
    "                             stop_words=stopwords, vocabulary=vocabulary)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "precision = cross_val_score(model, data.toarray(), target, cv=10, scoring='precision').mean()\n",
    "acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "\n",
    "print(str(len(vectorizer.get_feature_names())) + ': ' + 'precision(' + str(round(precision,2)) \n",
    "      + '), acc(' + str(round(acc,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outro -8.10070322691 comunidade\n",
      "outro -8.10070322691 fulo\n",
      "outro -8.02846293865 evangelho\n",
      "outro -7.97586418792 advogado\n",
      "outro -7.91363912711 fas\n",
      "outro -7.89800450082 pastor\n",
      "outro -7.89364444599 entrevista\n",
      "outro -7.8822569793 padre\n",
      "outro -7.85987611475 go\n",
      "outro -7.82261665925 oracao\n",
      "outro -7.81482705556 album\n",
      "outro -7.77486436848 autor\n",
      "outro -7.76548218148 numeros\n",
      "outro -7.76419758771 presidente\n",
      "outro -7.70299136354 sangue\n",
      "\n",
      "diario -4.22935883539 nao\n",
      "diario -4.87026259117 pra\n",
      "diario -4.9226554497 voce\n",
      "diario -5.28509092831 dia\n",
      "diario -5.28666217261 ja\n",
      "diario -5.31020657062 tudo\n",
      "diario -5.35384120852 so\n",
      "diario -5.36786923933 ser\n",
      "diario -5.39593473969 ate\n",
      "diario -5.45554107675 bem\n",
      "diario -5.47682802421 vida\n",
      "diario -5.47706558337 aqui\n",
      "diario -5.50857771893 sempre\n",
      "diario -5.55975295292 la\n",
      "diario -5.5603006546 sao\n"
     ]
    }
   ],
   "source": [
    "model.fit(data.toarray(),target)\n",
    "n = 15 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    print (class_labels[0], coef, feat)\n",
    "\n",
    "print()\n",
    "\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_names).to_csv('feature_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
