{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = pd.read_csv('corpus.csv.gz', compression='gzip')\n",
    "stopwords = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(916, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus = corpus[corpus['_golden'] == False]\n",
    "corpus = corpus[corpus['_trusted_judgments'] == 3]\n",
    "corpus = corpus.reset_index()\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fix labels to binary\n",
    "lb = preprocessing.LabelBinarizer(neg_label=1, pos_label=2)\n",
    "target = lb.fit_transform(corpus['qual_a_melhor_classificao_para_esse_texto'].values)\n",
    "c, r = target.shape\n",
    "target = target.reshape(c,)\n",
    "\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: f1(0.79), acc(0.69), precision(0.68), recall(0.95)\n",
      "600: f1(0.8), acc(0.69), precision(0.68), recall(0.95)\n",
      "700: f1(0.79), acc(0.69), precision(0.69), recall(0.94)\n",
      "800: f1(0.79), acc(0.69), precision(0.69), recall(0.94)\n",
      "900: f1(0.79), acc(0.69), precision(0.69), recall(0.94)\n",
      "1000: f1(0.8), acc(0.69), precision(0.69), recall(0.94)\n",
      "1100: f1(0.8), acc(0.7), precision(0.69), recall(0.94)\n",
      "1200: f1(0.8), acc(0.7), precision(0.69), recall(0.94)\n",
      "1300: f1(0.8), acc(0.7), precision(0.7), recall(0.95)\n",
      "1400: f1(0.8), acc(0.7), precision(0.69), recall(0.95)\n",
      "1500: f1(0.8), acc(0.71), precision(0.69), recall(0.95)\n",
      "1600: f1(0.8), acc(0.7), precision(0.69), recall(0.95)\n",
      "1700: f1(0.81), acc(0.71), precision(0.69), recall(0.96)\n",
      "1800: f1(0.8), acc(0.7), precision(0.69), recall(0.96)\n",
      "1900: f1(0.81), acc(0.71), precision(0.69), recall(0.97)\n",
      "2000: f1(0.8), acc(0.7), precision(0.69), recall(0.97)\n",
      "2100: f1(0.8), acc(0.7), precision(0.69), recall(0.96)\n",
      "2200: f1(0.8), acc(0.7), precision(0.69), recall(0.97)\n",
      "2300: f1(0.81), acc(0.7), precision(0.69), recall(0.97)\n",
      "2400: f1(0.8), acc(0.7), precision(0.69), recall(0.97)\n",
      "2500: f1(0.8), acc(0.7), precision(0.69), recall(0.97)\n",
      "2600: f1(0.8), acc(0.69), precision(0.68), recall(0.97)\n",
      "2700: f1(0.8), acc(0.7), precision(0.68), recall(0.97)\n",
      "2800: f1(0.8), acc(0.69), precision(0.68), recall(0.97)\n",
      "2900: f1(0.8), acc(0.69), precision(0.68), recall(0.97)\n",
      "3000: f1(0.8), acc(0.69), precision(0.68), recall(0.97)\n",
      "3100: f1(0.8), acc(0.69), precision(0.68), recall(0.97)\n",
      "3200: f1(0.8), acc(0.69), precision(0.68), recall(0.98)\n",
      "3300: f1(0.8), acc(0.68), precision(0.67), recall(0.98)\n",
      "3400: f1(0.79), acc(0.68), precision(0.67), recall(0.98)\n",
      "3500: f1(0.79), acc(0.68), precision(0.67), recall(0.98)\n",
      "3600: f1(0.79), acc(0.68), precision(0.67), recall(0.98)\n",
      "3700: f1(0.79), acc(0.68), precision(0.67), recall(0.98)\n",
      "3800: f1(0.79), acc(0.68), precision(0.67), recall(0.98)\n",
      "3900: f1(0.79), acc(0.68), precision(0.67), recall(0.98)\n",
      "4000: f1(0.79), acc(0.68), precision(0.67), recall(0.98)\n",
      "4100: f1(0.79), acc(0.68), precision(0.67), recall(0.98)\n",
      "4200: f1(0.79), acc(0.68), precision(0.67), recall(0.98)\n",
      "4300: f1(0.79), acc(0.68), precision(0.67), recall(0.98)\n",
      "4400: f1(0.79), acc(0.68), precision(0.67), recall(0.98)\n",
      "4500: f1(0.79), acc(0.67), precision(0.66), recall(0.98)\n",
      "4600: f1(0.79), acc(0.67), precision(0.66), recall(0.98)\n",
      "4700: f1(0.79), acc(0.67), precision(0.66), recall(0.98)\n",
      "4800: f1(0.79), acc(0.67), precision(0.66), recall(0.99)\n",
      "4900: f1(0.79), acc(0.67), precision(0.66), recall(0.99)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500,5000,100):\n",
    "    data = TfidfVectorizer(max_features=i, strip_accents='unicode', stop_words=stopwords).fit_transform(corpus.content)\n",
    "\n",
    "    f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "    acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "    recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "    precision = cross_val_score(model, data.toarray(), target, cv=10, scoring='precision').mean()\n",
    "    \n",
    "    print(str(i) + ': ' + 'f1(' + str(round(f1,2)) \n",
    "          + '), acc(' + str(round(acc,2)) \n",
    "          + '), precision(' + str(round(precision,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600: f1(0.8), acc(0.7), recall(0.95)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1600, ngram_range=(1,1), \n",
    "                             strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "\n",
    "print(str(1600) + ': ' + 'f1(' + str(round(f1,2)) \n",
    "      + '), acc(' + str(round(acc,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outro -8.3314569455 cu\n",
      "outro -8.3314569455 fila\n",
      "outro -8.3314569455 podcast\n",
      "outro -8.24566859883 batom\n",
      "outro -8.22148513779 delicia\n",
      "outro -8.20982899133 arroz\n",
      "outro -8.19055682527 estavamos\n",
      "outro -8.1897518293 barra\n",
      "outro -8.17309011169 pau\n",
      "outro -8.16502696116 farinha\n",
      "outro -8.15722875092 rsrs\n",
      "outro -8.14581052258 passe\n",
      "outro -8.14063547612 paris\n",
      "outro -8.13505403655 gostoso\n",
      "outro -8.13381430931 experiencias\n",
      "\n",
      "diario -4.79547441757 nao\n",
      "diario -5.45376614409 voce\n",
      "diario -5.61075264425 deus\n",
      "diario -5.81147719894 ser\n",
      "diario -5.90382633514 pra\n",
      "diario -5.93320886633 vida\n",
      "diario -5.96328795858 sao\n",
      "diario -5.98791821716 ja\n",
      "diario -6.0220089237 sobre\n",
      "diario -6.05815878779 bem\n",
      "diario -6.06380554662 tambem\n",
      "diario -6.07123447412 aqui\n",
      "diario -6.09793698797 tudo\n",
      "diario -6.12162943247 so\n",
      "diario -6.16403256526 livro\n"
     ]
    }
   ],
   "source": [
    "model.fit(data.toarray(),target)\n",
    "n = 15 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    print (class_labels[0], coef, feat)\n",
    "\n",
    "print()\n",
    "\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800: f1(0.8), acc(0.71), recall(0.94)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1600, ngram_range=(1,1), \n",
    "                             strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "model.fit(data.toarray(),target)\n",
    "n = 400 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    if feat not in vocabulary: \n",
    "        vocabulary.append(feat)\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    if feat not in vocabulary: \n",
    "        vocabulary.append(feat)\n",
    "    \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), strip_accents='unicode', \n",
    "                             stop_words=stopwords, vocabulary=vocabulary)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "\n",
    "print(str(len(vectorizer.get_feature_names())) + ': ' + 'f1(' + str(round(f1,2)) \n",
    "      + '), acc(' + str(round(acc,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outro -7.98182893797 cu\n",
      "outro -7.98182893797 fila\n",
      "outro -7.98182893797 podcast\n",
      "outro -7.87646370047 batom\n",
      "outro -7.82105757546 arroz\n",
      "outro -7.77939172504 barra\n",
      "outro -7.76418063199 estavamos\n",
      "outro -7.76335818658 rsrs\n",
      "outro -7.76150822999 pau\n",
      "outro -7.75791720278 experiencias\n",
      "outro -7.74621922094 jo\n",
      "outro -7.7457922427 paris\n",
      "outro -7.74431932426 delicia\n",
      "outro -7.74311631012 passe\n",
      "outro -7.71593987422 farinha\n",
      "\n",
      "diario -4.15502104615 nao\n",
      "diario -4.83747497164 voce\n",
      "diario -5.03017690904 deus\n",
      "diario -5.16257061156 ser\n",
      "diario -5.27183773675 pra\n",
      "diario -5.33186763052 vida\n",
      "diario -5.33248771114 sao\n",
      "diario -5.37117021303 ja\n",
      "diario -5.41227709869 sobre\n",
      "diario -5.42346262203 tambem\n",
      "diario -5.4280941392 aqui\n",
      "diario -5.43572795079 bem\n",
      "diario -5.4892861405 tudo\n",
      "diario -5.4984318563 so\n",
      "diario -5.54837421244 ate\n"
     ]
    }
   ],
   "source": [
    "model.fit(data.toarray(),target)\n",
    "n = 15 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    print (class_labels[0], coef, feat)\n",
    "\n",
    "print()\n",
    "\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_names).to_csv('feature_names.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
