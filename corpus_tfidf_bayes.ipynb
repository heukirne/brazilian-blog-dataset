{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = pd.read_csv('corpus.csv.gz', compression='gzip')\n",
    "stopwords = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 23)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus = corpus[corpus['_golden'] == False]\n",
    "#corpus = corpus[corpus['qual_a_melhor_classificao_para_esse_texto:confidence'] == 1]\n",
    "corpus = corpus.reset_index()\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix labels to binary\n",
    "lb = preprocessing.LabelBinarizer(neg_label=1, pos_label=2)\n",
    "target = lb.fit_transform(corpus['qual_a_melhor_classificao_para_esse_texto'].values)\n",
    "c, r = target.shape\n",
    "target = target.reshape(c,)\n",
    "\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "portuguese_stemmer = nltk.stem.RSLPStemmer()\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (portuguese_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: f1(0.7935), acc(0.6894), precision(0.6862), recall(0.9419)\n",
      "600: f1(0.796), acc(0.6935), precision(0.69), recall(0.9419)\n",
      "700: f1(0.801), acc(0.7024), precision(0.6976), recall(0.9418)\n",
      "800: f1(0.7975), acc(0.6974), precision(0.6947), recall(0.9371)\n",
      "900: f1(0.8005), acc(0.7024), precision(0.6975), recall(0.9403)\n",
      "1000: f1(0.8012), acc(0.7034), precision(0.6987), recall(0.9403)\n",
      "1100: f1(0.7987), acc(0.7004), precision(0.6964), recall(0.9372)\n",
      "1200: f1(0.7985), acc(0.7004), precision(0.6972), recall(0.9356)\n",
      "1300: f1(0.8006), acc(0.7025), precision(0.6977), recall(0.9403)\n",
      "1400: f1(0.8034), acc(0.7065), precision(0.7005), recall(0.9435)\n",
      "1500: f1(0.803), acc(0.7055), precision(0.6989), recall(0.945)\n",
      "1600: f1(0.8068), acc(0.7105), precision(0.7012), recall(0.9513)\n",
      "1700: f1(0.8074), acc(0.7104), precision(0.7003), recall(0.9545)\n",
      "1800: f1(0.8064), acc(0.7074), precision(0.6962), recall(0.9592)\n",
      "1900: f1(0.8026), acc(0.7004), precision(0.6905), recall(0.9592)\n",
      "2000: f1(0.8069), acc(0.7074), precision(0.6952), recall(0.9623)\n",
      "2100: f1(0.8069), acc(0.7064), precision(0.6936), recall(0.9655)\n",
      "2200: f1(0.8083), acc(0.7084), precision(0.6947), recall(0.967)\n",
      "2300: f1(0.8061), acc(0.7054), precision(0.6931), recall(0.9639)\n",
      "2400: f1(0.8056), acc(0.7044), precision(0.6923), recall(0.9639)\n",
      "2500: f1(0.8056), acc(0.7034), precision(0.6907), recall(0.9671)\n",
      "2600: f1(0.8071), acc(0.7054), precision(0.6913), recall(0.9702)\n",
      "2700: f1(0.8055), acc(0.7024), precision(0.6891), recall(0.9702)\n",
      "2800: f1(0.8034), acc(0.6994), precision(0.6875), recall(0.967)\n",
      "2900: f1(0.8055), acc(0.7013), precision(0.6874), recall(0.9733)\n",
      "3000: f1(0.8044), acc(0.6993), precision(0.6858), recall(0.9733)\n",
      "3100: f1(0.8036), acc(0.6973), precision(0.6838), recall(0.9749)\n",
      "3200: f1(0.8015), acc(0.6933), precision(0.6808), recall(0.9749)\n",
      "3300: f1(0.8015), acc(0.6933), precision(0.6808), recall(0.9749)\n",
      "3400: f1(0.7995), acc(0.6893), precision(0.6772), recall(0.9764)\n",
      "3500: f1(0.7985), acc(0.6873), precision(0.6757), recall(0.9764)\n",
      "3600: f1(0.797), acc(0.6843), precision(0.6736), recall(0.9764)\n",
      "3700: f1(0.7996), acc(0.6882), precision(0.6758), recall(0.9796)\n",
      "3800: f1(0.798), acc(0.6852), precision(0.6736), recall(0.9796)\n",
      "3900: f1(0.7971), acc(0.6832), precision(0.6722), recall(0.9796)\n",
      "4000: f1(0.7988), acc(0.6862), precision(0.674), recall(0.9811)\n",
      "4100: f1(0.7989), acc(0.6862), precision(0.6741), recall(0.9811)\n",
      "4200: f1(0.7976), acc(0.6842), precision(0.6729), recall(0.9796)\n",
      "4300: f1(0.7971), acc(0.6832), precision(0.6723), recall(0.9796)\n",
      "4400: f1(0.7966), acc(0.6822), precision(0.6717), recall(0.9796)\n",
      "4500: f1(0.7976), acc(0.6832), precision(0.6716), recall(0.9827)\n",
      "4600: f1(0.7973), acc(0.6822), precision(0.6704), recall(0.9843)\n",
      "4700: f1(0.7973), acc(0.6822), precision(0.6704), recall(0.9843)\n",
      "4800: f1(0.7973), acc(0.6822), precision(0.6704), recall(0.9843)\n",
      "4900: f1(0.7971), acc(0.6802), precision(0.6679), recall(0.989)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500,5000,100):\n",
    "    data = TfidfVectorizer(max_features=i, strip_accents='unicode', stop_words=stopwords).fit_transform(corpus.content)\n",
    "\n",
    "    f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "    acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "    recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "    precision = cross_val_score(model, data.toarray(), target, cv=10, scoring='precision').mean()\n",
    "    \n",
    "    print(str(i) + ': ' + 'f1(' + str(round(f1,4)) \n",
    "          + '), acc(' + str(round(acc,4)) \n",
    "          + '), precision(' + str(round(precision,4)) \n",
    "          + '), recall(' + str(round(recall,4)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600: f1(0.81), acc(0.71), recall(0.95)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1600, ngram_range=(1,1), \n",
    "                             strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "\n",
    "print(str(len(vectorizer.get_feature_names())) + ': ' + 'f1(' + str(round(f1,2)) \n",
    "      + '), acc(' + str(round(acc,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outro -8.38445320537 cu\n",
      "outro -8.38445320537 fila\n",
      "outro -8.38445320537 podcast\n",
      "outro -8.27412197174 delicia\n",
      "outro -8.26178137935 arroz\n",
      "outro -8.24618722404 estavamos\n",
      "outro -8.23383076293 barra\n",
      "outro -8.22442789639 pau\n",
      "outro -8.21081773286 rsrs\n",
      "outro -8.19486017784 passe\n",
      "outro -8.19053972054 paris\n",
      "outro -8.1895357691 experiencias\n",
      "outro -8.18679348584 gostoso\n",
      "outro -8.18541359566 mandou\n",
      "outro -8.1543390274 jb\n",
      "\n",
      "diario -4.77160886057 nao\n",
      "diario -5.40434529263 voce\n",
      "diario -5.59401028028 deus\n",
      "diario -5.79692213857 ser\n",
      "diario -5.90947887938 vida\n",
      "diario -5.9098196713 pra\n",
      "diario -5.94173508893 sao\n",
      "diario -5.97898827289 ja\n",
      "diario -5.99335615051 sobre\n",
      "diario -6.04260040949 tambem\n",
      "diario -6.04761185269 bem\n",
      "diario -6.07067402539 tudo\n",
      "diario -6.08741312006 aqui\n",
      "diario -6.09325653826 so\n",
      "diario -6.14125458545 dia\n"
     ]
    }
   ],
   "source": [
    "model.fit(data.toarray(),target)\n",
    "n = 15 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    print (class_labels[0], coef, feat)\n",
    "\n",
    "print()\n",
    "\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800: f1(0.81), acc(0.72), recall(0.95)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1600, ngram_range=(1,1), \n",
    "                             strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "model.fit(data.toarray(),target)\n",
    "n = 400 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    if feat not in vocabulary: \n",
    "        vocabulary.append(feat)\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    if feat not in vocabulary: \n",
    "        vocabulary.append(feat)\n",
    "    \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), strip_accents='unicode', \n",
    "                             stop_words=stopwords, vocabulary=vocabulary)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "\n",
    "print(str(len(vectorizer.get_feature_names())) + ': ' + 'f1(' + str(round(f1,2)) \n",
    "      + '), acc(' + str(round(acc,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outro -8.03938211904 cu\n",
      "outro -8.03938211904 fila\n",
      "outro -8.03938211904 podcast\n",
      "outro -7.87755730878 arroz\n",
      "outro -7.84285834641 barra\n",
      "outro -7.83346441799 pau\n",
      "outro -7.82218850845 rsrs\n",
      "outro -7.82194431472 estavamos\n",
      "outro -7.81760556428 experiencias\n",
      "outro -7.81403395644 mandou\n",
      "outro -7.7983243095 delicia\n",
      "outro -7.77246093595 paris\n",
      "outro -7.75845342645 passe\n",
      "outro -7.74309932011 saiba\n",
      "outro -7.73541633094 gostoso\n",
      "\n",
      "diario -4.14095885698 nao\n",
      "diario -4.79233326742 voce\n",
      "diario -5.03745839088 deus\n",
      "diario -5.1606207569 ser\n",
      "diario -5.27084299377 pra\n",
      "diario -5.31173627995 sao\n",
      "diario -5.31776964024 vida\n",
      "diario -5.36145346127 ja\n",
      "diario -5.38242925381 sobre\n",
      "diario -5.41668447114 tambem\n",
      "diario -5.43509443726 bem\n",
      "diario -5.46978656757 so\n",
      "diario -5.4701320931 aqui\n",
      "diario -5.47147489154 tudo\n",
      "diario -5.5278746145 ate\n"
     ]
    }
   ],
   "source": [
    "model.fit(data.toarray(),target)\n",
    "n = 15 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    print (class_labels[0], coef, feat)\n",
    "\n",
    "print()\n",
    "\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_names).to_csv('feature_names.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
