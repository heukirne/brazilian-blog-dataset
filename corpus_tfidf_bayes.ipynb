{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = pd.read_csv('corpus.csv.gz', compression='gzip')\n",
    "stopwords = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus = corpus[corpus['_golden'] == False]\n",
    "corpus = corpus[corpus['qual_a_melhor_classificao_para_esse_texto:confidence'] == 1]\n",
    "corpus = corpus[corpus['_trusted_judgments'] == 3]\n",
    "corpus = corpus.reset_index()\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# fix labels to binary\n",
    "lb = preprocessing.LabelBinarizer(neg_label=1, pos_label=2)\n",
    "target = lb.fit_transform(corpus['qual_a_melhor_classificao_para_esse_texto'].values)\n",
    "c, r = target.shape\n",
    "target = target.reshape(c,)\n",
    "\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: f1(0.83), acc(0.74), precision(0.73), recall(0.96)\n",
      "600: f1(0.83), acc(0.74), precision(0.73), recall(0.97)\n",
      "700: f1(0.83), acc(0.74), precision(0.73), recall(0.97)\n",
      "800: f1(0.84), acc(0.75), precision(0.73), recall(0.97)\n",
      "900: f1(0.83), acc(0.74), precision(0.73), recall(0.97)\n",
      "1000: f1(0.84), acc(0.75), precision(0.73), recall(0.98)\n",
      "1100: f1(0.84), acc(0.75), precision(0.73), recall(0.98)\n",
      "1200: f1(0.84), acc(0.75), precision(0.73), recall(0.98)\n",
      "1300: f1(0.83), acc(0.74), precision(0.73), recall(0.98)\n",
      "1400: f1(0.83), acc(0.74), precision(0.73), recall(0.98)\n",
      "1500: f1(0.83), acc(0.74), precision(0.72), recall(0.98)\n",
      "1600: f1(0.83), acc(0.74), precision(0.72), recall(0.99)\n",
      "1700: f1(0.83), acc(0.73), precision(0.72), recall(0.99)\n",
      "1800: f1(0.83), acc(0.74), precision(0.72), recall(0.99)\n",
      "1900: f1(0.83), acc(0.73), precision(0.71), recall(0.98)\n",
      "2000: f1(0.83), acc(0.73), precision(0.71), recall(0.99)\n",
      "2100: f1(0.83), acc(0.73), precision(0.71), recall(0.99)\n",
      "2200: f1(0.82), acc(0.72), precision(0.71), recall(0.99)\n",
      "2300: f1(0.82), acc(0.72), precision(0.7), recall(0.99)\n",
      "2400: f1(0.82), acc(0.72), precision(0.7), recall(0.99)\n",
      "2500: f1(0.82), acc(0.71), precision(0.7), recall(0.99)\n",
      "2600: f1(0.82), acc(0.71), precision(0.7), recall(0.99)\n",
      "2700: f1(0.82), acc(0.71), precision(0.7), recall(0.99)\n",
      "2800: f1(0.82), acc(0.71), precision(0.69), recall(0.99)\n",
      "2900: f1(0.82), acc(0.71), precision(0.69), recall(0.99)\n",
      "3000: f1(0.82), acc(0.71), precision(0.7), recall(0.99)\n",
      "3100: f1(0.82), acc(0.71), precision(0.7), recall(0.99)\n",
      "3200: f1(0.82), acc(0.71), precision(0.69), recall(0.99)\n",
      "3300: f1(0.82), acc(0.71), precision(0.69), recall(0.99)\n",
      "3400: f1(0.82), acc(0.71), precision(0.69), recall(1.0)\n",
      "3500: f1(0.82), acc(0.71), precision(0.69), recall(1.0)\n",
      "3600: f1(0.82), acc(0.7), precision(0.69), recall(1.0)\n",
      "3700: f1(0.82), acc(0.7), precision(0.69), recall(1.0)\n",
      "3800: f1(0.82), acc(0.7), precision(0.69), recall(1.0)\n",
      "3900: f1(0.81), acc(0.7), precision(0.69), recall(1.0)\n",
      "4000: f1(0.81), acc(0.7), precision(0.69), recall(1.0)\n",
      "4100: f1(0.81), acc(0.7), precision(0.69), recall(1.0)\n",
      "4200: f1(0.81), acc(0.7), precision(0.69), recall(1.0)\n",
      "4300: f1(0.81), acc(0.7), precision(0.68), recall(1.0)\n",
      "4400: f1(0.81), acc(0.7), precision(0.68), recall(1.0)\n",
      "4500: f1(0.81), acc(0.69), precision(0.68), recall(1.0)\n",
      "4600: f1(0.81), acc(0.69), precision(0.68), recall(1.0)\n",
      "4700: f1(0.81), acc(0.69), precision(0.68), recall(1.0)\n",
      "4800: f1(0.81), acc(0.69), precision(0.68), recall(1.0)\n",
      "4900: f1(0.81), acc(0.69), precision(0.68), recall(1.0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500,5000,100):\n",
    "    data = TfidfVectorizer(max_features=i, strip_accents='unicode', stop_words=stopwords).fit_transform(corpus.content)\n",
    "\n",
    "    f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "    acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "    recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "    precision = cross_val_score(model, data.toarray(), target, cv=10, scoring='precision').mean()\n",
    "    \n",
    "    print(str(i) + ': ' + 'f1(' + str(round(f1,2)) \n",
    "          + '), acc(' + str(round(acc,2)) \n",
    "          + '), precision(' + str(round(precision,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100: f1(0.84), acc(0.75), recall(0.98)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1100, ngram_range=(1,1), \n",
    "                             strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "\n",
    "print(str(len(vectorizer.get_feature_names())) + ': ' + 'f1(' + str(round(f1,2)) \n",
    "      + '), acc(' + str(round(acc,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outro -7.74422554188 argila\n",
      "outro -7.74422554188 chorei\n",
      "outro -7.72677609178 conversando\n",
      "outro -7.72176894178 look\n",
      "outro -7.7194942341 vc\n",
      "outro -7.70313391712 decidi\n",
      "outro -7.68997116739 aulas\n",
      "outro -7.68717598913 estavamos\n",
      "outro -7.68274339436 voltei\n",
      "outro -7.67484173552 antiga\n",
      "outro -7.67255919657 confesso\n",
      "outro -7.67072366437 pq\n",
      "outro -7.662440396 roupa\n",
      "outro -7.65687078523 bolsa\n",
      "outro -7.6444918728 manga\n",
      "\n",
      "diario -4.81661701098 nao\n",
      "diario -5.38366401185 voce\n",
      "diario -5.38778789457 deus\n",
      "diario -5.74824233259 ser\n",
      "diario -5.80614988863 vida\n",
      "diario -5.83152803134 senhor\n",
      "diario -5.84590104802 sao\n",
      "diario -5.93211670774 dia\n",
      "diario -5.94944110256 sobre\n",
      "diario -6.00312170711 tambem\n",
      "diario -6.00822310276 pra\n",
      "diario -6.01884216113 ja\n",
      "diario -6.03747837896 jesus\n",
      "diario -6.05110053498 vai\n",
      "diario -6.05155231736 bem\n"
     ]
    }
   ],
   "source": [
    "model.fit(data.toarray(),target)\n",
    "n = 15 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    print (class_labels[0], coef, feat)\n",
    "\n",
    "print()\n",
    "\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800: f1(0.84), acc(0.76), recall(0.97)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1100, ngram_range=(1,1), \n",
    "                             strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "model.fit(data.toarray(),target)\n",
    "n = 400 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    if feat not in vocabulary: \n",
    "        vocabulary.append(feat)\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    if feat not in vocabulary: \n",
    "        vocabulary.append(feat)\n",
    "    \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), strip_accents='unicode', \n",
    "                             stop_words=stopwords, vocabulary=vocabulary)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "\n",
    "print(str(len(vectorizer.get_feature_names())) + ': ' + 'f1(' + str(round(f1,2)) \n",
    "      + '), acc(' + str(round(acc,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outro -7.54447756112 argila\n",
      "outro -7.54447756112 chorei\n",
      "outro -7.51989601982 conversando\n",
      "outro -7.51942717661 vc\n",
      "outro -7.51509042099 look\n",
      "outro -7.5011637049 decidi\n",
      "outro -7.48683537044 aulas\n",
      "outro -7.47357190688 voltei\n",
      "outro -7.47021031394 estavamos\n",
      "outro -7.47005032276 pq\n",
      "outro -7.45595183547 antiga\n",
      "outro -7.45474685532 confesso\n",
      "outro -7.4502548188 bolsa\n",
      "outro -7.44049125571 manga\n",
      "outro -7.42998615274 roupa\n",
      "\n",
      "diario -4.49473800204 nao\n",
      "diario -5.0522965882 voce\n",
      "diario -5.12066520556 deus\n",
      "diario -5.44148558974 ser\n",
      "diario -5.50255986074 vida\n",
      "diario -5.53508327053 sao\n",
      "diario -5.54459172381 senhor\n",
      "diario -5.60108253307 dia\n",
      "diario -5.63829309562 sobre\n",
      "diario -5.68715235505 tambem\n",
      "diario -5.68911175799 pra\n",
      "diario -5.70453103937 ja\n",
      "diario -5.71444744759 vai\n",
      "diario -5.75375333114 bem\n",
      "diario -5.76293665763 pode\n"
     ]
    }
   ],
   "source": [
    "model.fit(data.toarray(),target)\n",
    "n = 15 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    print (class_labels[0], coef, feat)\n",
    "\n",
    "print()\n",
    "\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_names).to_csv('feature_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
