{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = pd.read_csv('corpus.csv.gz', compression='gzip')\n",
    "stopwords = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 23)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus = corpus[corpus['_golden'] == False]\n",
    "corpus = corpus[corpus['qual_a_melhor_classificao_para_esse_texto:confidence'] == 1]\n",
    "corpus = corpus.reset_index()\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix labels to binary\n",
    "lb = preprocessing.LabelBinarizer(neg_label=1, pos_label=2)\n",
    "target = lb.fit_transform(corpus['qual_a_melhor_classificao_para_esse_texto'].values)\n",
    "c, r = target.shape\n",
    "target = target.reshape(c,)\n",
    "\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "portuguese_stemmer = nltk.stem.RSLPStemmer()\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (portuguese_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500: f1(0.8354), acc(0.7529), precision(0.7391), recall(0.9624)\n",
      "600: f1(0.835), acc(0.751), precision(0.7351), recall(0.9682)\n",
      "700: f1(0.8356), acc(0.753), precision(0.7376), recall(0.9653)\n",
      "800: f1(0.8409), acc(0.7624), precision(0.7459), recall(0.9653)\n",
      "900: f1(0.8458), acc(0.7698), precision(0.7501), recall(0.9711)\n",
      "1000: f1(0.8408), acc(0.7624), precision(0.7458), recall(0.9654)\n",
      "1100: f1(0.8366), acc(0.7549), precision(0.7391), recall(0.9654)\n",
      "1200: f1(0.839), acc(0.7568), precision(0.736), recall(0.9771)\n",
      "1300: f1(0.8373), acc(0.7531), precision(0.7314), recall(0.9799)\n",
      "1400: f1(0.8439), acc(0.7643), precision(0.7397), recall(0.9828)\n",
      "1500: f1(0.8397), acc(0.7567), precision(0.7334), recall(0.9828)\n",
      "1600: f1(0.8329), acc(0.7456), precision(0.7264), recall(0.9771)\n",
      "1700: f1(0.8316), acc(0.7417), precision(0.7214), recall(0.9828)\n",
      "1800: f1(0.832), acc(0.7417), precision(0.7203), recall(0.9856)\n",
      "1900: f1(0.833), acc(0.7436), precision(0.7218), recall(0.9856)\n",
      "2000: f1(0.8306), acc(0.7399), precision(0.7198), recall(0.9828)\n",
      "2100: f1(0.8266), acc(0.7324), precision(0.7138), recall(0.9828)\n",
      "2200: f1(0.8265), acc(0.7323), precision(0.7137), recall(0.9828)\n",
      "2300: f1(0.8264), acc(0.7304), precision(0.7104), recall(0.9885)\n",
      "2400: f1(0.8221), acc(0.7229), precision(0.7059), recall(0.9855)\n",
      "2500: f1(0.8187), acc(0.7155), precision(0.6992), recall(0.9885)\n",
      "2600: f1(0.82), acc(0.7173), precision(0.6998), recall(0.9913)\n",
      "2700: f1(0.819), acc(0.7154), precision(0.6984), recall(0.9913)\n",
      "2800: f1(0.819), acc(0.7154), precision(0.6983), recall(0.9913)\n",
      "2900: f1(0.8184), acc(0.7136), precision(0.6959), recall(0.9943)\n",
      "3000: f1(0.8175), acc(0.7117), precision(0.6947), recall(0.9943)\n",
      "3100: f1(0.8175), acc(0.7117), precision(0.6947), recall(0.9943)\n",
      "3200: f1(0.8165), acc(0.7099), precision(0.6932), recall(0.9943)\n",
      "3300: f1(0.8156), acc(0.708), precision(0.6918), recall(0.9943)\n",
      "3400: f1(0.8155), acc(0.708), precision(0.6917), recall(0.9943)\n",
      "3500: f1(0.8145), acc(0.7061), precision(0.6903), recall(0.9943)\n",
      "3600: f1(0.8136), acc(0.7043), precision(0.6889), recall(0.9943)\n",
      "3700: f1(0.8145), acc(0.7061), precision(0.6903), recall(0.9943)\n",
      "3800: f1(0.8141), acc(0.7043), precision(0.6881), recall(0.9971)\n",
      "3900: f1(0.8126), acc(0.7024), precision(0.6875), recall(0.9943)\n",
      "4000: f1(0.8131), acc(0.7024), precision(0.6868), recall(0.9971)\n",
      "4100: f1(0.8131), acc(0.7024), precision(0.6868), recall(0.9971)\n",
      "4200: f1(0.8121), acc(0.7006), precision(0.6854), recall(0.9971)\n",
      "4300: f1(0.8103), acc(0.6969), precision(0.6828), recall(0.9971)\n",
      "4400: f1(0.8093), acc(0.6949), precision(0.6812), recall(0.9971)\n",
      "4500: f1(0.8073), acc(0.6912), precision(0.6785), recall(0.9971)\n",
      "4600: f1(0.8073), acc(0.6912), precision(0.6785), recall(0.9971)\n",
      "4700: f1(0.8064), acc(0.6893), precision(0.6771), recall(0.9971)\n",
      "4800: f1(0.8064), acc(0.6893), precision(0.6771), recall(0.9971)\n",
      "4900: f1(0.8064), acc(0.6893), precision(0.6771), recall(0.9971)\n"
     ]
    }
   ],
   "source": [
    "for i in range(500,5000,100):\n",
    "    data = TfidfVectorizer(max_features=i, strip_accents='unicode', stop_words=stopwords).fit_transform(corpus.content)\n",
    "\n",
    "    f1 = cross_val_score(model, data.toarray(), target, cv=10, scoring='f1').mean()\n",
    "    acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "    recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "    precision = cross_val_score(model, data.toarray(), target, cv=10, scoring='precision').mean()\n",
    "    \n",
    "    print(str(i) + ': ' + 'f1(' + str(round(f1,4)) \n",
    "          + '), acc(' + str(round(acc,4)) \n",
    "          + '), precision(' + str(round(precision,4)) \n",
    "          + '), recall(' + str(round(recall,4)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900: precision(0.75), acc(0.77), recall(0.97)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=900, ngram_range=(1,1), \n",
    "                             strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "precision = cross_val_score(model, data.toarray(), target, cv=10, scoring='precision').mean()\n",
    "acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "\n",
    "print(str(len(vectorizer.get_feature_names())) + ': ' + 'precision(' + str(round(precision,2)) \n",
    "      + '), acc(' + str(round(acc,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outro -7.66660825055 vc\n",
      "outro -7.66304838567 look\n",
      "outro -7.64695014061 decidi\n",
      "outro -7.61550917588 pq\n",
      "outro -7.60347721246 roupa\n",
      "outro -7.57225586371 00\n",
      "outro -7.57153146072 aulas\n",
      "outro -7.56967055418 mamae\n",
      "outro -7.54859480393 banheiro\n",
      "outro -7.53229558289 maximo\n",
      "outro -7.52708949314 horario\n",
      "outro -7.49453147458 estrada\n",
      "outro -7.49306493376 cm\n",
      "outro -7.48436758109 ficamos\n",
      "outro -7.48096061286 falo\n",
      "\n",
      "diario -4.60320035234 nao\n",
      "diario -5.16502189351 voce\n",
      "diario -5.26292503491 deus\n",
      "diario -5.5592107611 ser\n",
      "diario -5.62084727574 vida\n",
      "diario -5.66646626693 sao\n",
      "diario -5.6777980488 senhor\n",
      "diario -5.72753092471 dia\n",
      "diario -5.75305033976 sobre\n",
      "diario -5.80573797248 ja\n",
      "diario -5.80859557195 pra\n",
      "diario -5.80895899002 tambem\n",
      "diario -5.87279862689 bem\n",
      "diario -5.87528752613 vai\n",
      "diario -5.88612558354 jesus\n"
     ]
    }
   ],
   "source": [
    "model.fit(data.toarray(),target)\n",
    "n = 15 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    print (class_labels[0], coef, feat)\n",
    "\n",
    "print()\n",
    "\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800: precision(0.75), acc(0.77), recall(0.98)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=900, ngram_range=(1,1), \n",
    "                             strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "model.fit(data.toarray(),target)\n",
    "n = 400 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "vocabulary = []\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    if feat not in vocabulary: \n",
    "        vocabulary.append(feat)\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    if feat not in vocabulary: \n",
    "        vocabulary.append(feat)\n",
    "    \n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1), strip_accents='unicode', \n",
    "                             stop_words=stopwords, vocabulary=vocabulary)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "precision = cross_val_score(model, data.toarray(), target, cv=10, scoring='precision').mean()\n",
    "acc = cross_val_score(model, data.toarray(), target, cv=10, scoring='accuracy').mean()\n",
    "recall = cross_val_score(model, data.toarray(), target, cv=10, scoring='recall').mean()\n",
    "\n",
    "print(str(len(vectorizer.get_feature_names())) + ': ' + 'precision(' + str(round(precision,2)) \n",
    "      + '), acc(' + str(round(acc,2)) \n",
    "          + '), recall(' + str(round(recall,2)) + ')') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outro -7.59429862308 vc\n",
      "outro -7.59001189419 look\n",
      "outro -7.57368148819 decidi\n",
      "outro -7.54292604045 pq\n",
      "outro -7.528582848 roupa\n",
      "outro -7.4981474268 00\n",
      "outro -7.49679374177 aulas\n",
      "outro -7.49361204544 mamae\n",
      "outro -7.4694280946 banheiro\n",
      "outro -7.45299712197 maximo\n",
      "outro -7.44083181051 horario\n",
      "outro -7.41903858078 estrada\n",
      "outro -7.41282346653 cm\n",
      "outro -7.40534343004 ficamos\n",
      "outro -7.39444551039 loja\n",
      "\n",
      "diario -4.48420373812 nao\n",
      "diario -5.05351659125 voce\n",
      "diario -5.13579392755 deus\n",
      "diario -5.43694104236 ser\n",
      "diario -5.50804165338 vida\n",
      "diario -5.55459971551 sao\n",
      "diario -5.57192447499 senhor\n",
      "diario -5.59665784872 dia\n",
      "diario -5.64336783483 sobre\n",
      "diario -5.6880585823 ja\n",
      "diario -5.69142037497 tambem\n",
      "diario -5.69852604584 pra\n",
      "diario -5.76337732965 vai\n",
      "diario -5.76478595105 bem\n",
      "diario -5.77483979999 jesus\n"
     ]
    }
   ],
   "source": [
    "model.fit(data.toarray(),target)\n",
    "n = 15 \n",
    "\n",
    "class_labels = ['outro','diario']\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topn_class1 = sorted(zip(model.coef_[0], feature_names))[:n]\n",
    "topn_class2 = sorted(zip(model.coef_[0], feature_names))[-n:]\n",
    "\n",
    "for coef, feat in topn_class1:\n",
    "    print (class_labels[0], coef, feat)\n",
    "\n",
    "print()\n",
    "\n",
    "for coef, feat in reversed(topn_class2):\n",
    "    print (class_labels[1], coef, feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(feature_names).to_csv('feature_names.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
