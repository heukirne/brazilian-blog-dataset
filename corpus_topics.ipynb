{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = pd.read_csv('corpus.csv.gz', compression='gzip')\n",
    "stopwords = stopwords.words(\"portuguese\")\n",
    "corpus = corpus[corpus['qual_a_melhor_classificao_para_esse_texto:confidence'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        if topic_idx > 9:\n",
    "            break\n",
    "    print()\n",
    "    \n",
    "n_features = 1600\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "diarios = corpus[corpus['qual_a_melhor_classificao_para_esse_texto']=='diario']\n",
    "outro = corpus[corpus['qual_a_melhor_classificao_para_esse_texto']=='outro']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract features diarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, strip_accents='unicode',\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words=stopwords)\n",
    "tfidf = tfidf_vectorizer.fit_transform(diarios.content)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, strip_accents='unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords)\n",
    "tf = tf_vectorizer.fit_transform(diarios.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF Topics Diarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "nao ser tudo vida dia ja bem so sempre ate aqui sao la tempo porque ainda vou fazer tambem pessoas\n",
      "Topic #1:\n",
      "voce amor mim fara vai nao falta quero sera sabe sinto so feliz saber sentir coracao voz ser ama nunca\n",
      "Topic #2:\n",
      "pra jardim tava formar casa ai pau lindo semana nariz gente ne praia uns roupas voltei meio casinha comer ta\n",
      "Topic #3:\n",
      "foto vermelho cor juro marca ontem fica tambem estacao usei nao tirei umas sao lindo cores baixo tirar conseguia apaixonada\n",
      "Topic #4:\n",
      "livro leitura ler livros historia anos pedro impressao fato li tres super nesses lendo le sobre adoro gostei nessa legal\n",
      "Topic #5:\n",
      "cidade hotel estrada chegamos viagem vimos cartao fotos saimos pude delicia ja daqui andar fome aviao pegar dia nessa rio\n",
      "Topic #6:\n",
      "look acessorios bolsa estilo jeans calca looks cores modelos cor preta cabelo modelo combina usando azul tecidos opcoes pecas camisa\n",
      "Topic #7:\n",
      "natal noel papai arvore falou falei presentes desenho ganhar importante luzes ano epoca esquecer amor mamae entao sala espirito ser\n",
      "Topic #8:\n",
      "the to my you of in and is it michael night on your make escuridao musica deixa brasil musicas haha\n",
      "Topic #9:\n",
      "bolo colher acucar leite xicara farinha sobremesa massa po ovos margarina sopa receita ingredientes rosa criancas cha coloque formas gostoso\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topics Diarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "argila cabelos verde linha fios capilar nao facebook blog http www branca produtos in deixa tratamento acao https marca intenso\n",
      "Topic #1:\n",
      "creme maos dia nao fila cara pau tava mulher uso hoje casa ja porque gostei vida ate inverno rio voce\n",
      "Topic #2:\n",
      "brinco make voce nao rosto ser tipo usar so ideal pra maquiagem boca producao portanto vez formato tambem coisa pequeno\n",
      "Topic #3:\n",
      "nao manga ferramenta clique laco ate parte pra opcao cor site va tao sempre ser clicar ja detalhes voce bem\n",
      "Topic #4:\n",
      "veja trailer previsao estreia jeans nao corpo calca peca modelos aqui cintura combina basico voce 05 filmes preta ser todo\n",
      "Topic #5:\n",
      "pra nao voce bolo leite receita massa acucar xicara farinha dia formas colher sopa vou ja biscoitos po ate margarina\n",
      "Topic #6:\n",
      "sophia nao dia tudo so voce facebook https porque todos bom pessoas vc 11 ai pra ainda festa sempre mundo\n",
      "Topic #7:\n",
      "nao voce pra ja ser dia tudo so ate bem vida sao sempre ainda aqui tambem fazer tempo la mim\n",
      "Topic #8:\n",
      "the nao of to you saudade and night your it my tambem is in ser art musica anos deixa escuridao\n",
      "Topic #9:\n",
      "nao parque cidade tambem sao pude ate fazer pra bem ja porque bom principal tudo guerra ser vista agradavel todos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract features outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, strip_accents='unicode',\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words=stopwords)\n",
    "tfidf = tfidf_vectorizer.fit_transform(outro.content)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, strip_accents='unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords)\n",
    "tf = tf_vectorizer.fit_transform(outro.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF Topics Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "nao voce ser vida dia pra sao ja vai tudo bem pessoas sobre ate pode so ainda tambem ter porque\n",
      "Topic #1:\n",
      "deus jesus senhor palavra cristo nao evangelho salvacao oracao presenca amor graca igreja missao vida oracoes estar filho fe fome\n",
      "Topic #2:\n",
      "banda forro muido nota agradecer arnaldo fas breve blog amy decisao novidades propria obrigado pra cantar cantora confira todos felipe\n",
      "Topic #3:\n",
      "filme historia filmes comercial personagens organizacao imagens nota pretendo tema nascido pequena aparece seculo sobre acontece medidas trata onde inteiro\n",
      "Topic #4:\n",
      "meninas voces glitter namorados logo tons ne pra espero amiga blogspot fiz dar domingo dia deixar verde novidades ai quer\n",
      "Topic #5:\n",
      "facebook social rede esposa usuarios mulher foto empresa ainda mes orkut assassinato brasil the oficial matar original pena incrivel redes\n",
      "Topic #6:\n",
      "you now the cd and banda is all this down break world on your it tres tristeza re trilha music\n",
      "Topic #7:\n",
      "pele pes merecem foto area corpo passar estar aspecto serve creme dormir merece usar nota ideal super fique ambos pe\n",
      "Topic #8:\n",
      "numeros josue 14 senhor terra povo dez homens 13 deus atitudes cre porem dizem fe gloria 33 mel contra leite\n",
      "Topic #9:\n",
      "amigo amizade amigos verdadeiro alguem inimigo verdadeira coracao ti cre noivo olhar entra saude dificil tanto dor valor todos sucesso\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topics Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "nao oleo ser tambem voce vai todos mundo deus ainda disse hoje cada assim tudo sempre cozinha pra sao facebook\n",
      "Topic #1:\n",
      "nao pao creme tristeza bolo cha chocolate acucar receita colocar ate meio vossa preparo modo maquina ingredientes blue leite lo\n",
      "Topic #2:\n",
      "nao deus ser creme dia voce senhor pao bolo sempre amor vez porque hoje cha sobre voces todo mim disse\n",
      "Topic #3:\n",
      "nao so pra ser hoje lei deve senhora vida pode aqui tristeza ate vai dias ter jovem deus dois hora\n",
      "Topic #4:\n",
      "nao advogado policial pare voce parar ve ser sinal anos pessoas idade cidade estar ainda diferenca vou bem ja pode\n",
      "Topic #5:\n",
      "nao voce ser deus vida sao tambem ja dia sobre tudo ate bem pra pode todos tempo vai aqui ainda\n",
      "Topic #6:\n",
      "nao ser mente pode tudo tambem pessoas advogado vida mundo libertacao voce todas ja ter sente varias elenco trabalho parece\n",
      "Topic #7:\n",
      "you now the nao and banda cd all your on down break is this it novo world musica video to\n",
      "Topic #8:\n",
      "nao sinho voce cade mandou ah lady vem glitter bennett pra sobre tons cantora jazz aqui album ja ate http\n",
      "Topic #9:\n",
      "cidade idade nao mr natal numeros atual anos acham voce justin desenho ontario nome forca apelido desenhar josue canada desenhos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeled Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "portuguese_stemmer = nltk.stem.RSLPStemmer()\n",
    "class StemmedCountVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(CountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (portuguese_stemmer.stem(w) for w in analyzer(doc))\n",
    "    \n",
    "tf_vectorizer = CountVectorizer(strip_accents='unicode',\n",
    "                                stop_words=stopwords)\n",
    "\n",
    "def listTop(model, matrix, n_top):\n",
    "    termFreq = zip(model.get_feature_names(), np.asarray(matrix.sum(axis=0)).ravel())\n",
    "    for term, freq in sorted(termFreq, key=lambda term: term[1], reverse=True)[:n_top]:\n",
    "        print(str(freq) + ': ' + term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17: dia\n",
      "11: nao\n",
      "10: amor\n",
      "10: familia\n",
      "9: reflexoes\n",
      "9: trabalho\n",
      "9: vida\n",
      "8: blog\n",
      "7: cronicas\n",
      "6: ano\n",
      "6: brasil\n",
      "6: ferias\n",
      "6: moda\n",
      "6: viagem\n",
      "5: amigos\n",
      "5: coisas\n",
      "5: cozinha\n",
      "5: decoracao\n",
      "5: fazer\n",
      "5: filmes\n"
     ]
    }
   ],
   "source": [
    "tf = tf_vectorizer.fit_transform(diarios['labels'])\n",
    "listTop(tf_vectorizer, tf, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6: noticias\n",
      "5: facebook\n",
      "5: plus\n",
      "5: videos\n",
      "4: assistir\n",
      "4: cinema\n",
      "4: futebol\n",
      "4: livros\n",
      "4: politica\n",
      "4: size\n",
      "4: textos\n",
      "4: ufc\n",
      "3: artes\n",
      "3: conhecimento\n",
      "3: deus\n",
      "3: dicas\n",
      "3: drama\n",
      "3: entrevista\n",
      "3: espiritualidade\n",
      "3: esporte\n"
     ]
    }
   ],
   "source": [
    "tf = tf_vectorizer.fit_transform(outro['labels'])\n",
    "listTop(tf_vectorizer, tf, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18: dia\n",
      "16: nao\n",
      "9: voce\n",
      "8: semana\n",
      "8: ser\n",
      "6: 2011\n",
      "6: bem\n",
      "6: novo\n",
      "6: parte\n",
      "6: tudo\n",
      "6: vida\n",
      "5: hoje\n",
      "5: mim\n",
      "5: sempre\n",
      "5: sobre\n",
      "5: vez\n",
      "4: 2014\n",
      "4: anos\n",
      "4: bolo\n",
      "4: bom\n"
     ]
    }
   ],
   "source": [
    "tf = tf_vectorizer.fit_transform(diarios['title'])\n",
    "listTop(tf_vectorizer, tf, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9: nao\n",
      "6: parte\n",
      "6: sobre\n",
      "6: vida\n",
      "5: dia\n",
      "4: 10\n",
      "4: diz\n",
      "4: morte\n",
      "4: mundo\n",
      "4: ser\n",
      "3: capitulo\n",
      "3: copa\n",
      "3: deus\n",
      "3: esperanca\n",
      "3: facebook\n",
      "3: novo\n",
      "3: pouco\n",
      "3: social\n",
      "2: 2010\n",
      "2: 38\n"
     ]
    }
   ],
   "source": [
    "tf = tf_vectorizer.fit_transform(outro['title'])\n",
    "listTop(tf_vectorizer, tf, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
