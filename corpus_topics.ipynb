{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = pd.read_csv('corpus.csv.gz', compression='gzip')\n",
    "stopwords = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        if topic_idx > 9:\n",
    "            break\n",
    "    print()\n",
    "    \n",
    "n_features = 1600\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "diarios = corpus[corpus['qual_a_melhor_classificao_para_esse_texto']=='diario']\n",
    "outro = corpus[corpus['qual_a_melhor_classificao_para_esse_texto']=='outro']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract features diarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, strip_accents='unicode',\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words=stopwords)\n",
    "tfidf = tfidf_vectorizer.fit_transform(diarios.content)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, strip_accents='unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords)\n",
    "tf = tf_vectorizer.fit_transform(diarios.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF Topics Diarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "nao ja dia ser tudo bem ate vida sao sempre tempo tambem aqui so fazer ainda porque vou la pessoas\n",
      "Topic #1:\n",
      "voce quero mim vai sei sinto so sentir nao amo ser ama nunca sabe coracao pensar feliz sera saber voz\n",
      "Topic #2:\n",
      "pra aqui ai ta gente vc semana ne voces to pro tava cor casa so lindo olha postar jardim vai\n",
      "Topic #3:\n",
      "livro livros ler leitura romance historia autor li capa lendo autora paginas charlie lista capitulo forma resenha personagem traz anos\n",
      "Topic #4:\n",
      "leite bolo receita acucar chocolate xicara farinha massa bata sopa adicione colher ingredientes acrescente cha coloque forno misture sobremesa coco\n",
      "Topic #5:\n",
      "nao sei nada vai consigo acho mal pensar ninguem ha so posso sim pode mulher disse estar dizer quer problema\n",
      "Topic #6:\n",
      "pele base rosto po produto produtos usar tom cobertura maquiagem usando cor uso marcas bem fotos tipo foto aplicar deixa\n",
      "Topic #7:\n",
      "amor coracao amizade amo sentimento amar vida mim falar amigos sera cada guardar teste amada doce dor cor ha dizer\n",
      "Topic #8:\n",
      "contar estrelas ceu colher doces cantar fatos aprendi menor proprios dez silencio tentei mil conheci ouvi pequena confesso consegui tarde\n",
      "Topic #9:\n",
      "deus graca igreja senhor jesus palavra ceu vida jo familia tudo coracao proposito devemos ano propria quer pe podcast jovens\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topics Diarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "base pele cobertura aplicar cabelos conhecida marcas rosto 00 nao bem tom usando natural construir aqui total queda pode ate\n",
      "Topic #1:\n",
      "parque barra chocolate quatro sociais nao tocar ponto tambem voce etc aproveitar partes familia pensando ser comer vida fazer praca\n",
      "Topic #2:\n",
      "experiencia regra gastar ganhar mesa nao ouro aventura personagem regras importa agora escolhas peca relacao dinheiro espero sao escolher voce\n",
      "Topic #3:\n",
      "cores cor produtos linha argila cabelos pele produto pra verde site super shampoo nao sao ja compra bem http usando\n",
      "Topic #4:\n",
      "leite nao bolo bem look receita ate acucar chocolate velho massa arroz colher farinha xicara dia coloque ingredientes sopa sobremesa\n",
      "Topic #5:\n",
      "nao voce pra ja ser tudo bem so dia ate vida sao tambem sempre fazer tempo ainda aqui porque todos\n",
      "Topic #6:\n",
      "nao pra the voce ja amo vai vc ai ta dia so aqui to ser fazer clique bem http ate\n",
      "Topic #7:\n",
      "fio cm kg bolinha circulo grande estrutura bem bola mario abaixo parte 11 peso 12 passe fazer acrescente fique 2011\n",
      "Topic #8:\n",
      "batom labios efeito nao usar site comprei longa creme comprar cor surpresa pro ah sobre maquiagem preco comentarios super dica\n",
      "Topic #9:\n",
      "quero nao mascara fios make suor boca voce fazer piscina cabelos sol brilho rosto queda controle coco amo acao cabelo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extract features outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, strip_accents='unicode',\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words=stopwords)\n",
    "tfidf = tfidf_vectorizer.fit_transform(outros.content)\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, strip_accents='unicode',\n",
    "                                max_features=n_features,\n",
    "                                stop_words=stopwords)\n",
    "tf = tf_vectorizer.fit_transform(outros.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF Topics Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "nao ser vida ja sao so tudo bem dia voce ate ainda tambem pessoas pode vai sobre tempo porque assim\n",
      "Topic #1:\n",
      "deus senhor palavra estar oracao vida salvacao presenca povo oracoes cristo missao filhos graca biblia comigo igreja muitas filho evangelho\n",
      "Topic #2:\n",
      "livro livros li leitura historia autor ler sobre personagens editora paginas meninos volume sinopse ano edicao pouco narrativa gostei realmente\n",
      "Topic #3:\n",
      "voces meninas super pele base cabelo ne espero cor maquiagem colecao produtos bem aqui fiz linda amiga ola dia www\n",
      "Topic #4:\n",
      "musica banda album musicas amy cantora video paul the disco novo on versao and jazz lancado gaga fas of cantar\n",
      "Topic #5:\n",
      "jesus cristo jairo senhor caminho quero conosco pastor pes discipulos fe ti evangelho palavra morte vida nao mestre lider coracao\n",
      "Topic #6:\n",
      "voce tamara comportamento risos telefone verdade sim ah calca quer virus alguem pode vai sabe ve cabelo significa fazer erro\n",
      "Topic #7:\n",
      "sinto tristeza virtual procurar humano tao alma es corpo canto direitos mim amor acabar alcancar coracao 2011 sentimentos poder palavras\n",
      "Topic #8:\n",
      "filme filmes historia sobre cinema personagens nota versao protagonista paises bem comunicacao drama ver direcao resenha movimento imagens tema mundial\n",
      "Topic #9:\n",
      "pra gente vou credito endereco aqui telefone calca mim daqui linguagem desejo umas espelho coragem peito carro geral sexta ter\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=n_topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA Topics Outros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "you the banda album go now musica on and medico paul nao disco amy cd all is musicas of maquina\n",
      "Topic #1:\n",
      "nao aumento preta ponto pontos carreira legenda baixo baixos video arquivos arquivo verde comunicacao ser usando bem fazer usuarios servico\n",
      "Topic #2:\n",
      "negra cade vem ah http www fala voce roupa mail ajudar rei pra alunos rede youtube dormir es nao producao\n",
      "Topic #3:\n",
      "som ouvido questao intensidade velocidade onda nao frequencia sons porto antonio paulo alegre altura sonora sao oleo permite apenas possui\n",
      "Topic #4:\n",
      "nao vou existe musica ceu show pra ser porque aqui tudo sobre voltar sao 10 voce cheia vai rio tanto\n",
      "Topic #5:\n",
      "nao voce ser deus ja vida tambem sao bem sobre tudo so pra ate dia todos pode aqui fazer ainda\n",
      "Topic #6:\n",
      "nao aqui comeca dias so voces pra frio ta ver cara assim fotos demais tambem vou banho vai ja base\n",
      "Topic #7:\n",
      "nao cabelo creme vossa bem tambem cha bolo ofereco pao cor produtos linha receita ja super maquiagem vosso fazer acucar\n",
      "Topic #8:\n",
      "sombra usei nao canto olho tutorial linha jesus pra vida lapis cores dia usar ser gosta pego saiu escuro tons\n",
      "Topic #9:\n",
      "pesquisa questionario metodos design academia nao usuario teste ja recursos dados aqui mercado tudo usuarios profissionais vida so pratica projetos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "lda.fit(tf)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
