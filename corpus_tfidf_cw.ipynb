{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import oll\n",
    "\n",
    "corpus = pd.read_csv('corpus.csv.gz', compression='gzip')\n",
    "stopwords = stopwords.words(\"portuguese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(534, 23)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus = corpus[corpus['_golden'] == False]\n",
    "corpus = corpus[corpus['qual_a_melhor_classificao_para_esse_texto:confidence'] == 1]\n",
    "corpus = corpus.reset_index()\n",
    "corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['diario' 'outro']\n",
      "0    1\n",
      "1   -1\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# fix labels to binary\n",
    "def classFit(x):\n",
    "    if x['qual_a_melhor_classificao_para_esse_texto'] == \"diario\":\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "corpus['class'] = corpus.apply(classFit,axis=1)\n",
    "target = corpus['class'].values\n",
    "\n",
    "print(corpus['qual_a_melhor_classificao_para_esse_texto'].values[:2])\n",
    "print(corpus['class'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "portuguese_stemmer = nltk.stem.RSLPStemmer()\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (portuguese_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## manual 10-fold cross-validation\n",
    "kf = KFold(n_splits=2, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800: acc(0.758426966292), prec(0.752552778975), rec(0.936540327167)\n",
      "900: acc(0.76404494382), prec(0.755231560892), rec(0.942528351119)\n",
      "1000: acc(0.765917602996), prec(0.74983692107), rec(0.959488843542)\n",
      "1100: acc(0.762172284644), prec(0.747248379601), rec(0.956494831566)\n",
      "1200: acc(0.769662921348), prec(0.750909090909), rec(0.965476867494)\n",
      "1300: acc(0.769662921348), prec(0.750909090909), rec(0.965476867494)\n",
      "1400: acc(0.765917602996), prec(0.746596411844), rec(0.96847087947)\n",
      "1500: acc(0.762172284644), prec(0.743106364566), rec(0.96847087947)\n",
      "1600: acc(0.756554307116), prec(0.736842105263), rec(0.971264175559)\n",
      "1700: acc(0.758426966292), prec(0.736329979115), rec(0.976850767738)\n",
      "1800: acc(0.756554307116), prec(0.734683075294), rec(0.976850767738)\n",
      "1900: acc(0.760299625468), prec(0.737991266376), rec(0.976850767738)\n"
     ]
    }
   ],
   "source": [
    "for i in range(800,2000,100):\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    \n",
    "    data = TfidfVectorizer(max_features=i, strip_accents='unicode', stop_words=stopwords).fit_transform(corpus.content)\n",
    "\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        model = oll.oll(\"CW\", C=2)\n",
    "        \n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted = model.predict(X_test)\n",
    "\n",
    "        accuracy.append(accuracy_score(y_test, predicted))\n",
    "        precision.append(precision_score(y_test, predicted))\n",
    "        recall.append(recall_score(y_test, predicted))\n",
    "\n",
    "    print(str(i) + ': acc(' + str(np.mean(accuracy)) \n",
    "          + '), prec(' + str(np.mean(precision))\n",
    "          + '), rec(' + str(np.mean(recall)) + ')'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: acc(0.769662921348), prec(0.755509468745), rec(0.953902251363)\n",
      "6: acc(0.771535580524), prec(0.753542178542), rec(0.962282139631)\n",
      "11: acc(0.767790262172), prec(0.753424657534), rec(0.95350081959)\n",
      "16: acc(0.769662921348), prec(0.755079385028), rec(0.95350081959)\n",
      "21: acc(0.775280898876), prec(0.760135924392), rec(0.95350081959)\n",
      "26: acc(0.771535580524), prec(0.760249670063), rec(0.944920215435)\n",
      "31: acc(0.767790262172), prec(0.759242275358), rec(0.939333623256)\n",
      "36: acc(0.769662921348), prec(0.763536170974), rec(0.933546315191)\n",
      "41: acc(0.762172284644), prec(0.760144436922), rec(0.925166426923)\n",
      "46: acc(0.756554307116), prec(0.757347605225), rec(0.919579834744)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1200, strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "for c in range(1,51,5):\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "\n",
    "    for train_index, test_index in kf.split(data):\n",
    "        model = oll.oll(\"CW\", C=c)\n",
    "        \n",
    "        X_train, X_test = data[train_index], data[test_index]\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        predicted = model.predict(X_test)\n",
    "\n",
    "        accuracy.append(accuracy_score(y_test, predicted))\n",
    "        precision.append(precision_score(y_test, predicted))\n",
    "        recall.append(recall_score(y_test, predicted))\n",
    "\n",
    "    print(str(c) + ': acc(' + str(np.mean(accuracy)) \n",
    "          + '), prec(' + str(np.mean(precision))\n",
    "          + '), rec(' + str(np.mean(recall)) + ')'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900: acc(0.76404494382), prec(0.755231560892), rec(0.942528351119)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1200, strip_accents='unicode', stop_words=stopwords)\n",
    "data = vectorizer.fit_transform(corpus.content)\n",
    "\n",
    "accuracy = []\n",
    "precision = []\n",
    "recall = []\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "\n",
    "    model = oll.oll(\"CW\", C=2)\n",
    "    \n",
    "    X_train, X_test = data[train_index], data[test_index]\n",
    "    y_train, y_test = target[train_index], target[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    predicted = model.predict(X_test)\n",
    "\n",
    "    accuracy.append(accuracy_score(y_test, predicted))\n",
    "    precision.append(precision_score(y_test, predicted))\n",
    "    recall.append(recall_score(y_test, predicted))\n",
    "\n",
    "print(str(len(vectorizer.get_feature_names())) + ': acc(' + str(np.mean(accuracy)) \n",
    "          + '), prec(' + str(np.mean(precision))\n",
    "          + '), rec(' + str(np.mean(recall)) + ')'\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()\n",
    "pd.DataFrame(feature_names).to_csv('feature_names_cw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
