{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LSTM RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#容易上手的keras，为了保持和Aaron老师用的DL库一致...\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Dropout,Activation\n",
    "from keras.models import model_from_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vocab_dim = 100\n",
    "maxlen = 100\n",
    "n_iterations = 5 \n",
    "n_exposures = 10\n",
    "window_size = 7\n",
    "batch_size = 32\n",
    "n_epoch = 4\n",
    "input_length = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def classFit(x):\n",
    "    if x['qual_a_melhor_classificao_para_esse_texto'] == \"diario\":\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def loadfile():\n",
    "    corpus = pd.read_csv('corpus.csv.gz', compression='gzip')\n",
    "    corpus = corpus[corpus['qual_a_melhor_classificao_para_esse_texto:confidence'] == 1]\n",
    "    corpus.reset_index()\n",
    "    corpus['class'] = corpus.apply(classFit,axis=1)\n",
    "    y = corpus['class'].values\n",
    "\n",
    "    combined= corpus.content\n",
    "\n",
    "    return combined,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import unicodedata\n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "def tokenizer(text):\n",
    "    ''' Simple Parser converting each document to lower-case, then\n",
    "        removing the breaks for new lines and finally splitting on the\n",
    "        whitespace\n",
    "    '''\n",
    "    return_text = []\n",
    "    for sentence in text:\n",
    "        reg_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "        tokens = reg_tokenizer.tokenize(sentence)\n",
    "        return_text.append([remove_accents(w.lower()) for w in tokens])\n",
    "        \n",
    "    return return_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_dictionaries(model=None,\n",
    "                        combined=None):\n",
    "    ''' Function does are number of Jobs:\n",
    "        1- Creates a word to index mapping\n",
    "        2- Creates a word to vector mapping\n",
    "        3- Transforms the Training and Testing Dictionaries\n",
    "\n",
    "    '''\n",
    "    if (combined is not None) and (model is not None):\n",
    "        gensim_dict = Dictionary()\n",
    "        \n",
    "        w2v = dict(zip(model.wv.index2word, model.wv.syn0))\n",
    "        \n",
    "        gensim_dict.doc2bow(w2v.keys(),\n",
    "                            allow_update=True)\n",
    "        w2indx = {v: k+1 for k, v in gensim_dict.items()}\n",
    "        w2vec = {word: model[word] for word in w2indx.keys()}\n",
    "\n",
    "        def parse_dataset(combined):\n",
    "            ''' Words become integers\n",
    "            '''\n",
    "            data=[]\n",
    "            for sentence in combined:\n",
    "                new_txt = []\n",
    "                for word in sentence:\n",
    "                    try:\n",
    "                        new_txt.append(w2indx[word])\n",
    "                    except:\n",
    "                        new_txt.append(0)\n",
    "                data.append(new_txt)\n",
    "            return data\n",
    "        combined=parse_dataset(combined)\n",
    "        combined= sequence.pad_sequences(combined, maxlen=maxlen)\n",
    "        return w2indx, w2vec,combined\n",
    "    else:\n",
    "        print ('No data provided...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def word2vec_train(combined):    \n",
    "    model = Word2Vec(combined, size=vocab_dim, workers=16, iter=10, negative=20)\n",
    "    # trim memory\n",
    "    model.init_sims(replace=True)\n",
    "    \n",
    "    model.save('Word2vec_model.pkl')\n",
    "    index_dict, word_vectors,combined = create_dictionaries(model=model,combined=combined)\n",
    "    return   index_dict, word_vectors,combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_data(index_dict,word_vectors,combined,y):\n",
    "\n",
    "    n_symbols = len(index_dict) + 1\n",
    "    embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "    for word, index in index_dict.items():\n",
    "        embedding_weights[index, :] = word_vectors[word]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(combined, y, test_size=0.2)\n",
    "    print (x_train.shape,y_train.shape)\n",
    "    return n_symbols,embedding_weights,x_train,y_train,x_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test):\n",
    "    print ('Defining a Simple Keras Model...')\n",
    "    model = Sequential()  # or Graph or whatever\n",
    "    model.add(Embedding(output_dim=vocab_dim,\n",
    "                        input_dim=n_symbols,\n",
    "                        mask_zero=True,\n",
    "                        weights=[embedding_weights],\n",
    "                        input_length=input_length))  # Adding Input Length\n",
    "    \n",
    "    model.add(LSTM(units=50, recurrent_activation=\"hard_sigmoid\", activation=\"sigmoid\"))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    print ('Compiling the Model...')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    print (\"Train...\")\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=n_epoch,verbose=0, validation_data=(x_test, y_test))\n",
    "\n",
    "    print (\"Evaluate...\")\n",
    "    score = model.evaluate(x_test, y_test,\n",
    "                                batch_size=batch_size)\n",
    "\n",
    "    yaml_string = model.to_yaml()\n",
    "    with open('lstm.yml', 'w') as outfile:\n",
    "        outfile.write( yaml.dump(yaml_string, default_flow_style=True) )\n",
    "    model.save_weights('lstm.h5')\n",
    "    print ('Test score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    print ('Loading Data...')\n",
    "    combined,y=loadfile()\n",
    "    print (len(combined),len(y))\n",
    "    print ('Tokenising...')\n",
    "    combined = tokenizer(combined)\n",
    "    print ('Training a Word2vec model...')\n",
    "    index_dict, word_vectors,combined=word2vec_train(combined)\n",
    "    print ('Setting up Arrays for Keras Embedding Layer...')\n",
    "    n_symbols,embedding_weights,x_train,y_train,x_test,y_test=get_data(index_dict, word_vectors,combined,y)\n",
    "    print (x_train.shape,y_train.shape)\n",
    "    train_lstm(n_symbols,embedding_weights,x_train,y_train,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def input_transform(string):\n",
    "    words = []\n",
    "    reg_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokens = reg_tokenizer.tokenize(string)\n",
    "    words.append([remove_accents(w.lower()) for w in tokens])\n",
    "    \n",
    "    words=np.array(words).reshape(1,-1)\n",
    "    model=Word2Vec.load('Word2vec_model.pkl')\n",
    "    _,_,combined=create_dictionaries(model,words)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def lstm_predict(string):\n",
    "    print ('loading model......')\n",
    "    with open('lstm.yml', 'r') as f:\n",
    "        yaml_string = yaml.load(f)\n",
    "    model = model_from_yaml(yaml_string)\n",
    "\n",
    "    print ('loading weights......')\n",
    "    model.load_weights('lstm.h5')\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',metrics=['accuracy'])\n",
    "    data=input_transform(string)\n",
    "    data.reshape(1,-1)\n",
    "    #print data\n",
    "    result=model.predict_classes(data)\n",
    "    if result[0][0]==1:\n",
    "        print (string,' positive')\n",
    "    else:\n",
    "        print (string,' negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data...\n",
      "551 551\n",
      "Tokenising...\n",
      "Training a Word2vec model...\n",
      "Setting up Arrays for Keras Embedding Layer...\n",
      "(440, 100) (440,)\n",
      "(440, 100) (440,)\n",
      "Defining a Simple Keras Model...\n",
      "Compiling the Model...\n",
      "Train...\n",
      "Evaluate...\n",
      " 32/111 [=======>......................] - ETA: 0sTest score: [0.71661183372274173, 0.0]\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model......\n",
      "loading weights......\n",
      "1/1 [==============================] - 0s\n",
      "Nunca imaginei ver isso um dia na mídia, mas para a nossa felicidade aconteceu: tricotar está na moda! Agora é comum ver um grupo de jovens ou senhoras batendo papo e tricotando. Ótima opção para relaxar. Sei tricotar o básico... o ponto tricô e o ponto laçada e meia. Fiz há um tempo estes singelos sapatinhos para o frio. Lembro de tricotar desde criança para as minhas barbies e tentando terminar meu primeiro cachecol. Eu adorava tricotar, mas me faltava paciência na época. Ainda assim, como qualquer arte, recomendo. É ótimo para quem deseja relaxar, principalmente as pessoas que não trabalham nesta área. Afinal, quem não gosta de usar algo lindo e dizer para todos que foi você mesma quem fez? Abaixo deixo um vídeo que fala exatamente desta nova moda. Vale a Dica! Fazer tricô é a nova mania em São Paulo E você, o que mais gosta de tricotar? Como aprendeu? Compartilhe a sua experiência!  negative\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "combined,y=loadfile()\n",
    "\n",
    "idx = 4\n",
    "string = combined[idx]\n",
    "lstm_predict(string)\n",
    "\n",
    "print(y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
